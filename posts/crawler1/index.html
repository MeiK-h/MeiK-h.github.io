<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>MeiK&#39;s blog  | 如何写一个爬虫 - 第一篇</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.58.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="如何写一个爬虫 - 第一篇" />
<meta property="og:description" content="从零开始实现一个爬虫。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meik2333.com/posts/crawler1/" />
<meta property="article:published_time" content="2019-07-30T20:38:06+00:00" />
<meta property="article:modified_time" content="2019-07-30T20:38:06+00:00" />
<meta itemprop="name" content="如何写一个爬虫 - 第一篇">
<meta itemprop="description" content="从零开始实现一个爬虫。">


<meta itemprop="datePublished" content="2019-07-30T20:38:06&#43;00:00" />
<meta itemprop="dateModified" content="2019-07-30T20:38:06&#43;00:00" />
<meta itemprop="wordCount" content="515">



<meta itemprop="keywords" content="爬虫,Python," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="如何写一个爬虫 - 第一篇"/>
<meta name="twitter:description" content="从零开始实现一个爬虫。"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://meik2333.com" class="f3 fw2 hover-white no-underline white-90 dib">
      MeiK&#39;s blog
    </a>
    <div class="flex-l items-center">
      

      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">如何写一个爬虫 - 第一篇</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-07-30T20:38:06Z">July 30, 2019</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>从零开始实现一个爬虫。</p>

<blockquote>
<p>To eat when you’re hungry, to sleep when you’re tired, to take a break when you’re bored, to work on projects that seem fun and interesting. &mdash;&mdash; <a href="http://www.aaronsw.com/weblog/productivity">Aaron Swartz</a></p>
</blockquote>

<p>关于如何写爬虫，网上已经有很多资源了，随便搜索一下就可以获得很多教程。跟着那些教程，很快就可以达到爬取豆瓣、微博等网站的水平。利用无头浏览器等技术，可以轻松的爬取网上绝大部分可见的资源。</p>

<p>然而，回想起大一刚开始学习爬虫时候的我，当时我还在使用 Python2 ，时常因为错误的编码而陷入困境，在网上的各种博客里复制来的代码运行倒是能运行，但其实哪一句什么意思都不知道，踩了数不清的坑之后才勉强入门。但如果过去的我能够看到现在网上铺天盖地的爬虫教程，我踩的坑会因此而变少吗？</p>

<p>对我来说，答案是否定的。区别可能是，以前我在 CSDN 上复制 Python2 + <code>urllib</code> 的代码，现在我在知乎专栏复制 Python3 + <code>requests</code> 的代码。</p>

<p>因此，我决定自己写博客来讲解如何写爬虫。考虑到现在我的博客日访问量还是个位数，因此我大概率是写给自己看的。当然这并没有什么关系，就当作是写给大一时候的我自己了。</p>

<h2 id="所需基础">所需基础</h2>

<p>如果可以的话，我肯定是想能让零基础的人也能学会，然而我的能力它不允许啊……</p>

<p>因此，在学习之前，你需要以下前置技能：</p>

<ol>
<li>Python 基础</li>
<li>TCP/IP 基础（稍有了解即可）</li>
<li>HTML 基础</li>
<li>JavaScript 基础</li>
</ol>

<p>你需要以下环境：</p>

<ol>
<li>能够联网的电脑</li>
<li>Python 3.6 +</li>
<li>不需要第三方库</li>
</ol>

<p>在学习完之后，你应该会获得以下技能：</p>

<ol>
<li>基础的爬虫技能</li>
</ol>

<p>本系列博客以完成一个完整可用的爬虫框架为目标，读者需要通过框架的代码来了解爬虫。我会尽可能的以渐进的方式编写代码与撰写博客，读者最好能够跟着教程实际敲一下代码，以此来加深理解。</p>

<h2 id="起步">起步</h2>

<p>包括 Web 网站、各种小程序、软件等在内，现行的网络世界中产生的大部分流量都是基于 HTTP 协议（HyperText Transfer Protocol ，超文本传输协议）的。因此，为了了解爬虫，我们首先要对 HTTP 协议有个大概的认识。</p>

<p><strong>注意</strong>：如果没有特别指明，我们所说的 HTTP 协议均为 <a href="https://tools.ietf.org/html/rfc2616">RFC2616</a> 中所定义的 HTTP/1.1。HTTP/2 及之后版本的变动很大，且目前还只在小部分范围内被使用，因此此处不考虑这些较新的协议，之后将单独对其进行讲解。</p>

<p>HTTP 基于 TCP 协议——一种可靠的传输协议。因为它偏离了我们要讲解的主题，如果我们不会在这里对其进行详细介绍，可以看一下我之前的几篇博客，了解一下 TCP：</p>

<ol>
<li><a href="/posts/tcp-connection/">《TCP 的三次握手与四次挥手》</a></li>
<li><a href="/posts/tcp-status/">《TCP 连接的流程与状态转换》</a></li>
<li><a href="/posts/tcp-sliding-window/">《TCP 的滑动窗口》</a></li>
<li><a href="/posts/tcp-congestion-control/">《TCP 的拥塞控制》</a></li>
</ol>

<p>如果你确实没有相关的基础也没关系，你可以用“连接的双方互相（可以同时）向对方发送字符串的协议”来理解 TCP。双方是指客户端方（主动发起连接方）和服务端方（接受连接方），在我们的语境中，就是我们的爬虫和被爬取的网站服务器。</p>

<p>因此，建立在 TCP 基础上的 HTTP，可以简单的理解为用户与网站交换字符串。网站服务器处理请求和浏览器渲染网页，其实都是要先进行解析字符串的。关于 HTTP 协议的字符串格式，同样在 <a href="https://tools.ietf.org/html/rfc2616">RFC2616</a> 有定义。当然，翻阅全英文的文档是让人有点头疼的，因此我从维基百科上直接复制了一份描述：</p>

<blockquote>
<ul>
<li>请求行，例如： <code>GET /logo.gif HTTP/1.1</code> 或状态码行，例如： <code>HTTP/1.1 200 OK</code></li>
<li>HTTP 头字段</li>
<li>空行</li>
<li>可选的 HTTP 报文主体数据</li>
</ul>

<p>请求/状态行和标题必须以 <code>&lt;CR&gt;</code> <code>&lt;LF&gt;</code> 结尾（即回车后跟一个换行符）。 空行必须只包含 <code>&lt;CR&gt;</code> <code>&lt;LF&gt;</code> ，而不能包含其他空格。</p>
</blockquote>

<p>这里的 <code>&lt;CR&gt;</code> <code>&lt;LF&gt;</code> 写在程序里就是 <code>&quot;\r\n&quot;</code> ，打印出来的话表现为两个换行。除了首行外，请求和响应的格式要求是相同的。</p>

<p><strong>请求格式</strong></p>

<pre><code class="language-bsh">[请求方法] [Path] [协议版本]
[header 字段]: [header 值]
...
[header 字段]: [header 值]

[数据]
</code></pre>

<p><strong>请求样例</strong></p>

<pre><code class="language-http">GET /index.html HTTP/1.1
Host: example.com
</code></pre>

<p><strong>响应格式</strong></p>

<pre><code>[协议版本] [响应状态码] [响应消息]
[header 字段]: [header 值]
...
[header 字段]: [header 值]

[数据]
</code></pre>

<p><strong>响应样例</strong></p>

<pre><code class="language-http">HTTP/1.1 200 OK
Date: Sun, 10 Oct 2010 23:26:07 GMT
Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g
Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT
ETag: &quot;45b6-834-49130cc1182c0&quot;
Accept-Ranges: bytes
Content-Length: 13
Connection: close
Content-Type: text/html

Hello world!
</code></pre>

<h2 id="编码实现一个基本的请求">编码实现一个基本的请求</h2>

<p>先来介绍一个网站： <a href="https://httpbin.org">https://httpbin.org</a> ，这是一个用于测试请求的网站，是测试爬虫的最合适的工具之一。比如你访问 <a href="https://httpbin.org/get">https://httpbin.org/get</a> ，它会把你请求的 <code>headers</code> 信息、 <code>url</code> 、参数等信息都返回给你。</p>

<p><img src="/images/crawler1/WX20190731-184700.png" alt="" /></p>

<p>因为 https 需要处理证书加解密，我们现在还无法做到，因此我们使用这个网站的 http 版本 <a href="http://httpbin.org">http://httpbin.org</a> ，使用 Python ，请求其 <code>/get</code> 接口。</p>

<p>在 Python 中，创建一个 TCP 连接可以使用 socket 库，这是一个内置库，<a href="https://docs.python.org/3/library/socket.html">这里</a>是它的文档。</p>

<pre><code class="language-python">import socket

sock = socket.socket()
sock.connect((&quot;httpbin.org&quot;, 80))

request = &quot;GET /get HTTP/1.0\r\n&quot; + \
          &quot;Host: httpbin.org\r\n&quot; + \
          &quot;\r\n&quot;
sock.send(request.encode(&quot;ascii&quot;))

response = b&quot;&quot;
chunk = sock.recv(4096)
while chunk:
    response += chunk
    chunk = sock.recv(4096)
sock.close()

print(response.decode())
</code></pre>

<p>这段代码的意思是，我们构建了一段请求字符串如下：</p>

<pre><code class="language-http">GET /get HTTP/1.0
Host: httpbin.org
</code></pre>

<p>创建了一个与 httpbin.org 的 TCP 连接，并将请求字符串发送给网站服务器。创建了一个 <code>response</code> 的变量来存储服务器发给我们的数据。因为协议中规定了 HTTP 的编码为 ascii ，而 Python3 默认的编码为 Unicode ，因此在发送与接收时我们都需要进行转码。</p>

<p>需要注意的是， <code>sock.recv(4096)</code> 返回的是读取到的服务器给我们发送的数据，如果数据还没有收到， <code>sock.recv</code> 并不会返回空，而是会一直阻塞在这里，等待数据到达。只有当连接已经断开或者连接出错的时候， <code>sock.recv</code> 才会返回一个空字符串。</p>

<p>有的同学可能会发现，我们上面说我们的教程基于 <code>HTTP/1.1</code> ，这里发送的却是 <code>HTTP/1.0</code> 。这是因为，在 <code>HTTP/1.1</code> 中， HTTP header 中新增了字段 <code>Content-Length</code> ，服务器会在响应数据的 header 中添加这个字段，用于表示响应数据的长度，正常的浏览器应该识别这个字段，并在服务器传输了足够长度的数据后主动关闭连接。</p>

<p>客户端主动关闭连接可以有效的减少服务器 TIME_WAIT 积压的问题，对这个问题有兴趣的同学可以看一下我的博客<a href="https://meik2333.com/2018/08/17/TCP-%E7%9A%84%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%8E%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/">《TCP 的三次握手与四次挥手》</a>和<a href="https://meik2333.com/2018/08/19/TCP-%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2/">《TCP 连接的流程与状态转换》</a>。转回我们的教程，我们还不想在现在的代码中添加主动关闭的功能，因此我们直接发送 <code>HTTP/1.0</code> 的请求给服务器，来规避这个问题。</p>

<p>保存代码到本地并执行，我们可以直观的看到其效果：</p>

<p><img src="/images/crawler1/WX20190731-194635@2x.png" alt="" /></p>

<h2 id="封装成函数">封装成函数</h2>

<p>我们的爬虫当然是不会只访问 httpbin.org 的，我们需要把通用的代码封装起来，供后续调用。</p>

<pre><code class="language-python">import socket


def fetch(url: str) -&gt; str:
    if url.startswith(&quot;http://&quot;) is False:
        raise ValueError(&quot;url must start with `http://`&quot;)

    _tmp = url[7:].split(&quot;/&quot;, 1)
    if len(_tmp) &gt; 1:
        host = _tmp[0]
        path = &quot;/&quot; + _tmp[1]
    else:
        host = _tmp[0]
        path = &quot;/&quot;

    sock = socket.socket()
    sock.connect((host, 80))

    request = &quot;\r\n&quot;.join((f&quot;GET {path} HTTP/1.0&quot;, f&quot;Host: {host}&quot;, &quot;\r\n&quot;))
    sock.send(request.encode(&quot;ascii&quot;))

    response = b&quot;&quot;
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    sock.close()

    return response.decode()
</code></pre>

<p>我们将爬虫通用的逻辑封装成了一个叫 <code>fetch</code> 的函数，这个函数接受一个 <code>url</code> 参数，请求对应的网站并返回其响应。我们可以在其他地方使用这个函数。</p>

<p>将代码保存为 <code>fetch.py</code> ，在同一个目录下打开一个终端（ Windows 下可以 PowerShell ），启动 python 并输入以下代码：</p>

<pre><code class="language-python">&gt;&gt;&gt; from fetch import fetch
&gt;&gt;&gt; resp = fetch('http://httpbin.org/get')
&gt;&gt;&gt; resp
'HTTP/1.1 200 OK\r\nAccess-Control-Allow-Credentials: true\r\nAccess-Control-Allow-Origin: *\r\nContent-Type: application/json\r\nDate: Wed, 31 Jul 2019 12:07:58 GMT\r\nReferrer-Policy: no-referrer-when-downgrade\r\nServer: nginx\r\nX-Content-Type-Options: nosniff\r\nX-Frame-Options: DENY\r\nX-XSS-Protection: 1; mode=block\r\nContent-Length: 146\r\nConnection: Close\r\n\r\n{\n  &quot;args&quot;: {}, \n  &quot;headers&quot;: {\n    &quot;Host&quot;: &quot;httpbin.org&quot;\n  }, \n  &quot;origin&quot;: &quot;36.110.78.251, 36.110.78.251&quot;, \n  &quot;url&quot;: &quot;https://httpbin.org/get&quot;\n}\n'
</code></pre>

<p>验证通过，我们的函数已经可以请求网站了！</p>

<h2 id="创建框架">创建框架</h2>

<p>虽然我们的函数已经可以实现请求了，但我们要做的是一个爬虫框架，需要更高的适用性。为了将我们的函数封装成框架，我们首先创建以下的目录结构：</p>

<pre><code>ZeroCrawler
    |
    |------ ZeroCrawler
    |         |-------- __init__.py
    |         |-------- fetch.py
    |      
    |------ tests
              |-------- test_fetch.py
</code></pre>

<p>文件内容分别如下：</p>

<p><strong><code>__init__.py</code></strong></p>

<pre><code class="language-python">from .fetch import fetch
</code></pre>

<p><strong><code>test_fetch.py</code></strong></p>

<pre><code class="language-python">import unittest

from ZeroCrawler import fetch


class TestFetch(unittest.TestCase):
    def test_fetch(self):
        resp = fetch(&quot;http://httpbin.org/get&quot;)
        self.assertTrue(resp.startswith(&quot;HTTP/1.1 200 OK&quot;))


if __name__ == &quot;__main__&quot;:
    unittest.main()
</code></pre>

<p>我们添加了 tests 来测试我们的代码，以保证我们代码的可靠与稳定。现在我们的框架已经有了基本的雏形，在下一章里，我们将会把 ZeroCrawler 改造成一个可以安装使用的库。</p>

<h2 id="总结">总结</h2>

<p>这一篇里，我们大概了解了一下爬虫的原理，并且写代码实现了一个最基础的爬虫框架。</p>

<p>本章的所有源代码可以在 GitHub ：<a href="https://github.com/MeiK2333/ZeroCrawler/tree/092d5109e1493d9d820db1f852922f377e0f20c1">ZeroCrawler Version 0.0.1</a> 中找到。如果有意见或者建议，可以通过评论或者 <a href="https://github.com/MeiK2333/ZeroCrawler/issues">GitHub Issues</a> 来告诉我。我们下期再见。</p>

<p><img src="/images/crawler1/40D98F7E9BDEEC8BB55AADA12BB1EB98.gif" style="max-width: 200px"></p><ul class="pa0">
  
   <li class="list">
     <a href="/tags/%E7%88%AC%E8%99%AB" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">爬虫</a>
   </li>
  
   <li class="list">
     <a href="/tags/python" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Python</a>
   </li>
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/posts/fix-bug/">我是如何找到一个 BUG 的</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/get-web-qqmusic-download-link/">通过网页 QQ 音乐获取音乐的下载地址</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/django-dynamically-changes-file-upload-path/">Django 动态更改文件上传路径</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/python-generates-dynamic-ascii-painting/">Python 生成动态字符画</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/unit-testing-in-python/">Python 中的单元测试（ unittest 的基础用法）</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/django-tutorial/">Django tutorial</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/django-model-field-reference-field-options/">Django Model field reference - Field options</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/django-model-field-reference-field-types/">Django Model field reference - Field types</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://meik2333.com" >
    &copy; 2019 MeiK&#39;s blog
  </a>
    <div>










</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
